{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905946d5",
   "metadata": {},
   "source": [
    "# Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b587a",
   "metadata": {},
   "source": [
    "## Тема: Flow internalization in crypto market\n",
    "\n",
    "Цель работы в том, чтобы найти оптимальный способ сдвигания цен.\n",
    "\n",
    "Модель на основе обучения с подкреплением (DQN).\n",
    "\n",
    "Общая структура модели описана в статье по [ссылке](https://www.notion.so/Internalization-907cd477edc449c79a935263167ba49e#574c2dfa049247a29e7cfde5cf5fae54).\n",
    "\n",
    "Предлагаю взять базовый каркас модели, описанный на страницах 1-3 и обогатить его чем-нибудь со страницы 4.\n",
    "\n",
    "Стоит использовать практический подход к решению задачи, т.е. вместо выписывания и решения уравнения Бэллмана специфицировать и обучить RL модель на реальных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436f49c",
   "metadata": {},
   "source": [
    "# Презентация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d79bf1",
   "metadata": {},
   "source": [
    "## Price Skewing / Ценовой сдвиг"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fade1e",
   "metadata": {},
   "source": [
    "## Простая модель для изменения курса доллара к рублю\n",
    "$A, B, M, S$ - рыночная ставка, ask, mid, spread\n",
    "\n",
    "$a, b$ - наша bid и ask\n",
    "\n",
    "Дискретное время $t$. Размер order - $s, q$.\n",
    "\n",
    "Вероятность выполнения (execution probability):\n",
    "\n",
    "$p(a)=exp(k*(A-a)/S), k>0$\n",
    "\n",
    "$PnL=(a-M)*p(a)+(M-b)*p(b)$\n",
    "\n",
    "$Risk=|Q|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d8644",
   "metadata": {},
   "source": [
    "Целевая функция в момент времени $t$:\n",
    "$$F = PnL(a,b) - r*Risk(Q)$$\n",
    "$a, b$ --- управляющие переменные, $Q$ --- переменная состояния.\n",
    "\n",
    "Эволюция состояния:\n",
    "Если мы продадим $q$ в момент времени $t$, то $Q(t+1) = Q(t)-q$.\n",
    "\n",
    "Value function:\n",
    "$$ V \\left( Q(t) \\right) = \\max_{a, b} \\left[ \\sum_t \\left( \\beta^t \\times F(t+1) \\right) \\right]$$\n",
    "Уравнение Беллмана:\n",
    "$$V \\left( Q(t) \\right) = \\max_{a,b} \\left[ F + \\beta \\times V \\left( Q(t+1) \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1e466",
   "metadata": {},
   "source": [
    "## Возможные осложнения:\n",
    "\n",
    "● Размер заказа не всегда равен $q$. В действительности $q_a$ и $q_b$ контролируют возрастные переменные.\n",
    "\n",
    "● Если мы разместим заказ размером $q$, то мы можем получить сделку размером $<q$.\n",
    "\n",
    "● Время непрерывно\n",
    "\n",
    "● Середина не является постоянной\n",
    "\n",
    "● Вероятность выполнения $p(a)$ и $p(b)$ может быть более сложной\n",
    "\n",
    "● Рыночный спред развивается с течением времени\n",
    "\n",
    "● У нас есть несколько валют. В этом случае $Q$ - это вектор. И $Risk(Q)$ может\n",
    "выглядеть так: $Risk(Q) = (Q^T \\times \\Omega \\times Q)^u$, $u>0$\n",
    "\n",
    "● $A(t)$, $B(t)$ и $P(t)$ могут быть процессами с памятью\n",
    "\n",
    "● У нас могут быть транзакционные издержки, например, комиссионные\n",
    "\n",
    "● Мы можем ввести задержку исполнения. Например, мы принимаем решение в момент времени $t$, но\n",
    "цена будет изменена только при $t + 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74293cbe",
   "metadata": {},
   "source": [
    "## Вычислительные эксперименты\n",
    "\n",
    "Данные лежат вот [тут](https://drive.google.com/drive/folders/1uVRv8l_x0dqWZdODL89usxaOzh5XRImK)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819f941",
   "metadata": {},
   "source": [
    "## Запуск модели\n",
    "\n",
    "### Общий подход следующий.\n",
    "Мы итерируемся по данным от начала и до конца. Модель выставляет биды и офера в стакан, отменяет их или переставляет на более предпочтительные места. По ходу дела она собирает фидбэк и учится ставить ордера по оптимальным ценам.\n",
    "\n",
    "Для каждого выставленного ордера надо понять, исполнится он или нет. Ордер может исполниться по двум причинам.\n",
    "\n",
    "1) Если рынок пошел в его сторону. Например, мы выставили офер по $61.25$. А через $1$ мс рыночный бид стал $61.30$. Для этого анализа понадобятся цены из файлов $tob*$.\n",
    "\n",
    "2) Если на рынке произошла сделка по цене, пересекающей наш ордер. Например, мы выставили офер по $61.25$. А через $1$ мс произошла сделка по цене $61.26$. Это значит, что эта сделка вызвана ордером, который смэтчился бы с нашим ордером.\n",
    "\n",
    "При итерировании по данным не требуется данные никак группировать. Просто применяем все апдейты по очереди. Данных много, поэтому код на python скорее всего будет работать довольно медленно. Лучше писать симуляцию на каком-то компилируемом языке на ваш выбор. Либо как-то ускорять python (например [вот так](https://github.com/exaloop/codon)).\n",
    "\n",
    "Модель при принятии решения может использовать как данные из сырого биржевого стакана, так и синтетические котировки. Синтетика кратко описана [вот тут](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/d4d72bdb-ce29-4b26-9deb-93239db48022/Synthetic_liquidity.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230207%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230207T161651Z&X-Amz-Expires=86400&X-Amz-Signature=e02511b9e677f9981304370d66ba6c2c4426d98d1311ebe3926bb3dcff8bf459&X-Amz-SignedHeaders=host&response-content-disposition=filename%3D%22Synthetic%2520liquidity.pdf%22&x-id=GetObject). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee43e873",
   "metadata": {},
   "source": [
    "# Реализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e5446",
   "metadata": {},
   "source": [
    "## Материалы по python\n",
    "\n",
    "[Ссылка](https://www.tensorflow.org/guide/keras/save_and_serialize?hl=ru) на керас в тензорфлоу\n",
    "\n",
    "[Ссылка](https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/DdpgAgent): from rl.agents import DDPGAgent. Вот [это](https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/DqnAgent) не стоит делать, т.к. конечное кол-во состояний.\n",
    "\n",
    "[Ссылка](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential): from rl.memory import SequentialMemory tf.keras.Sequential\n",
    "\n",
    "[Ссылка](https://github.com/e-dorigatti/inverted-pendulum/blob/master/ddpg.py) на процесс Орнштейна-Улинбека: from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455a7bc",
   "metadata": {},
   "source": [
    "## Размышления о реализации\n",
    "\n",
    "Лучше реализовывать DDPGAgent, а не классический DQN с конечным количеством действий.\n",
    "\n",
    "Если реализовывать классический DQN, то можно руководствоваться следующими соображениями: \n",
    "1. Торговать бесконечное кол-во денег мы не можем (денег во всем мире ограничено);\n",
    "2. Торговать $\\sqrt{2}$ денег мы тоже не можем;\n",
    "3. Можно \"аппроксимировать\" наши сделки, округлив их, например до 100\\$.\n",
    "4. Ограничить сделки сверху (соотвественно снизу для продаж), например, 10 млн.\\$\n",
    "\n",
    "НО! Тогда получим сетку $\\sim 2*10^6*$(кол-во валют), что уже является большим пространством состояний. Поэтому лучше реализовывать как модель с непрерывным простраством действий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247e802",
   "metadata": {},
   "source": [
    "## Проверка среды на работоспособность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you using Colab?\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "    !sudo apt-get install xvfb ffmpeg x11-utils\n",
    "    !pip install 'gym==0.17.3'\n",
    "    !pip install 'imageio==2.4.0'\n",
    "    !pip install PILLOW\n",
    "    !pip install 'pyglet==1.3.2'\n",
    "    !pip install pyvirtualdisplay\n",
    "    !pip install 'tf-agents==0.12.0'\n",
    "    !pip install imageio-ffmpeg\n",
    "    print(\"Note: done for Colab!\")\n",
    "else:\n",
    "    print(\"Note: done for PC!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What version of Python do we have?\n",
    "import sys\n",
    "\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import platform\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "print(f\"Numpy {np.__version__}\")\n",
    "print(f\"Gym {gym.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3fcd4",
   "metadata": {},
   "source": [
    "## Реализация DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dddf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ddpg import actor_network\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4056d5bc",
   "metadata": {},
   "source": [
    "Тестирование среды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'market_maker-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "i = 0\n",
    "env.verbose = False\n",
    "while not done:\n",
    "    i += 1\n",
    "    state, reward, done, _ = env.step([1, 1, 2000, 3000])\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1e089",
   "metadata": {},
   "source": [
    "Гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long should training run?\n",
    "num_iterations = 3000\n",
    "# How often should the program provide an update.\n",
    "log_interval = 500\n",
    "\n",
    "# How many initial random steps, before training start, to\n",
    "# collect initial data.\n",
    "initial_collect_steps = 1000\n",
    "# How many steps should we run each iteration to collect\n",
    "# data from.\n",
    "collect_steps_per_iteration = 50\n",
    "# How much data should we store for training examples.\n",
    "replay_buffer_max_length = 100000\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# How many episodes should the program use for each evaluation.\n",
    "num_eval_episodes = 100\n",
    "# How often should an evaluation occur.\n",
    "eval_interval = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3571a4b",
   "metadata": {},
   "source": [
    "Instantiate the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd83e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'market_maker-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3710a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53196ef7",
   "metadata": {},
   "source": [
    "Создаем две среды:\n",
    "1. Для обучения\n",
    "2. Для оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d60ab",
   "metadata": {},
   "source": [
    "Создаем нейронную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_fc_layers = (400, 300)\n",
    "critic_obs_fc_layers = (400,)\n",
    "critic_action_fc_layers = None\n",
    "critic_joint_fc_layers = (300,)\n",
    "ou_stddev = 0.2\n",
    "ou_damping = 0.15\n",
    "target_update_tau = 0.05\n",
    "target_update_period = 5\n",
    "dqda_clipping = None\n",
    "td_errors_loss_fn = tf.compat.v1.losses.huber_loss\n",
    "gamma = 0.995\n",
    "reward_scale_factor = 1.0\n",
    "gradient_clipping = None\n",
    "\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-3\n",
    "debug_summaries = False\n",
    "summarize_grads_and_vars = False\n",
    "\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "\n",
    "actor_net = actor_network.ActorNetwork(\n",
    "    train_env.time_step_spec().observation,\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=actor_fc_layers,\n",
    ")\n",
    "\n",
    "critic_net_input_specs = (train_env.time_step_spec().observation,\n",
    "                          train_env.action_spec())\n",
    "\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "    critic_net_input_specs,\n",
    "    observation_fc_layer_params=critic_obs_fc_layers,\n",
    "    action_fc_layer_params=critic_action_fc_layers,\n",
    "    joint_fc_layer_params=critic_joint_fc_layers,\n",
    ")\n",
    "\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    ou_stddev=ou_stddev,\n",
    "    ou_damping=ou_damping,\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    dqda_clipping=dqda_clipping,\n",
    "    td_errors_loss_fn=td_errors_loss_fn,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    debug_summaries=debug_summaries,\n",
    "    summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "    train_step_counter=global_step)\n",
    "tf_agent.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c75b31",
   "metadata": {},
   "source": [
    "### Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39f3de9",
   "metadata": {},
   "source": [
    "### Сбор данных\n",
    "\n",
    "Теперь выполняем случайную политику в среде в течение нескольких шагов, записывая данные в буфер воспроизведения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a595a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = \\\n",
    "        environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(\\\n",
    "        time_step, action_step,\\\n",
    "        next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    buffer.add_batch(traj)\n",
    "\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\\\n",
    "    train_env.time_step_spec(),\\\n",
    "    train_env.action_spec())\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989cb37c",
   "metadata": {},
   "source": [
    "### Обучение агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52172d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using\n",
    "# TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy,\n",
    "                                num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "    # Collect a few steps using collect_policy and \n",
    "    # save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, tf_agent.collect_policy, replay_buffer)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the\n",
    "    # agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = tf_agent.train(experience).loss\n",
    "\n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, tf_agent.policy,\n",
    "                                        num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe54c5",
   "metadata": {},
   "source": [
    "### Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6efe86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDE OUTPUT\n",
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename, 'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "\n",
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
    "    filename = filename + \".mp4\"\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = eval_env.reset()\n",
    "            video.append_data(eval_py_env.render())\n",
    "            while not time_step.is_last():\n",
    "                action_step = policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                video.append_data(eval_py_env.render())\n",
    "    return embed_mp4(filename)\n",
    "\n",
    "\n",
    "create_policy_eval_video(tf_agent.policy, \"trained-agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675feb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604f93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4494c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c11386",
   "metadata": {},
   "source": [
    "## Итерирование по данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3183c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tob_iterator = df_tob.iterrows()\n",
    "trade_iterator = df_trade.iterrows()\n",
    "\"\"\"\n",
    "Применение:\n",
    "a = next(tob_iterator)\n",
    "row_df_tob = pd.DataFrame(next(tob_iterator)[1]).T\n",
    "выдает строчку из df_tob\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a82590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcce4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45598f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow_ver_07.02.23)",
   "language": "python",
   "name": "tensorflow_ver_07.02.23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
